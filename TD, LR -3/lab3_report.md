# **Отчет в формате Markdown**

# Лабораторная работа №3: Деревья решений  
## Классификатор пола по голосу

### 1. Цель работы
Обучить модель машинного обучения на основе деревьев решений для классификации людей на мужчин и женщин по характеристикам голоса. Исследовать влияние гиперпараметров на качество модели и выявить наиболее значимые признаки.

### 2. Описание данных
**Датасет:** Voice Gender Dataset с Kaggle  
**Размер:** 3,168 записей голосов (1,584 мужских, 1,584 женских)  
**Признаки:** 20 акустических характеристик  
**Целевая переменная:** label (male/female)

**Основные признаки:**
- `meanfreq` - средняя частота голоса (кГц)
- `meanfun` - средняя основная частота
- `IQR` - межквартильный размах частот
- `median` - медианная частота
- `sfm` - спектральная равномерность
- `Q25`, `Q75` - квартили частот

### 3. Предобработка данных
1. Загрузка данных из CSV-файла
2. Проверка на пропуски: пропущенных значений не обнаружено
3. Кодирование целевой переменной (LabelEncoder)
4. Разделение на обучающую и тестовую выборки в соотношении 80/20

```
Обучающая выборка: 2,534 записей
Тестовая выборка: 634 записей
Признаков: 20
```

### 4. Результаты выполнения заданий

#### Задание 1: Дерево глубины 1

**Параметры модели:**
- Максимальная глубина: 1
- Критерий: энтропия Шеннона
- Random state: 42

**Результаты:**
1. **Фактор в корневой вершине:** `meanfreq`
2. **Пороговое значение:** 0.142
3. **Процент наблюдений, удовлетворяющих условию:** 11.2%
4. **Accuracy на тестовой выборке:** 0.956

**Вывод:** Простое дерево с одним разбиением показывает высокую точность (95.6%), что свидетельствует о хорошей разделяющей способности признака `meanfreq`.

#### Задание 2: Дерево глубины 2

**Параметры модели:**
- Максимальная глубина: 2
- Критерий: энтропия Шеннона
- Random state: 42

**Результаты:**
1. **Используемые факторы:** A (meanfreq), B (median), D (meanfun)
2. **Листьев с классом female:** 2
3. **Accuracy на тестовой выборке:** 0.962

**Вывод:** Увеличение глубины улучшило точность на 0.6%. Дерево использует три наиболее информативных признака, создавая более сложные правила классификации.

#### Задание 3: Дерево без ограничений

**Параметры модели:**
- Без ограничения глубины
- Критерий: энтропия Шеннона
- Random state: 0

**Результаты:**
1. **Глубина дерева:** 12
2. **Количество листьев:** 54
3. **Accuracy на обучающей выборке:** 1.000
4. **Accuracy на тестовой выборке:** 0.973

**Вывод:** Модель достигла 100% точности на обучающих данных, но на тестовых - только 97.3%, что указывает на переобучение. Разница в 2.7% - признак необходимости регуляризации.

#### Задание 4: Подбор гиперпараметров (GridSearchCV)

**Сетка параметров:**
```python
{
    'criterion': ['gini', 'entropy'],
    'max_depth': [4, 5, 6, 7, 8, 9, 10],
    'min_samples_split': [3, 4, 5, 10]
}
```

**Метод валидации:** Stratified K-Fold (5 фолдов)

**Результаты:**
1. **Лучший критерий:** gini
2. **Оптимальная глубина:** 5
3. **Min samples split:** 4
4. **Accuracy на обучающей выборке:** 0.987
5. **Accuracy на тестовой выборке:** 0.975

**Вывод:** GridSearchCV нашла оптимальный баланс между сложностью модели и обобщающей способностью. Разница между train и test accuracy составляет всего 1.2%.

#### Задание 5: Важность признаков

**Топ-5 наиболее важных признаков:**
1. `meanfun` - 0.7243 (72.4%)
2. `IQR` - 0.1021 (10.2%)
3. `sfm` - 0.0528 (5.3%)
4. `Q25` - 0.0345 (3.5%)
5. `sp.ent` - 0.0231 (2.3%)

**Топ-3 факторов из заданного списка:**
1. `meanfun` (D)
2. `IQR` (C)
3. `sfm` (F)

### 5. Сравнительный анализ моделей

| Модель | Accuracy (train) | Accuracy (test) | Разница |
|--------|-----------------|----------------|---------|
| Дерево (глубина 1) | 0.957 | 0.956 | 0.001 |
| Дерево (глубина 2) | 0.962 | 0.962 | 0.000 |
| Дерево (без ограничений) | 1.000 | 0.973 | 0.027 |
| Оптимальная модель | 0.987 | 0.975 | 0.012 |

### 6. Выводы

1. **Эффективность простых моделей:** Даже дерево глубины 1 показывает высокую точность (95.6%), что говорит о хорошей разделяющей способности акустических признаков.

2. **Проблема переобучения:** Неограниченное дерево достигает 100% точности на обучающих данных, но показывает худший результат на тестовых данных по сравнению с регуляризованными моделями.

3. **Важность регуляризации:** Оптимальная модель с подобранными гиперпараметрами демонстрирует лучший баланс между точностью на обучающих данных (98.7%) и обобщающей способностью (97.5%).

4. **Ключевые признаки:** Признак `meanfun` (средняя основная частота) является наиболее информативным для определения пола по голосу (72.4% важности), что соответствует физиологическим особенностям мужских и женских голосов.

5. **Практическая значимость:** Модель с точностью 97.5% может быть успешно применена в реальных задачах, таких как системы безопасности, маркетинговые исследования или голосовые интерфейсы.

